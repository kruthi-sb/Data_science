{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M4 : Data Transforms and Dimantionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "Techinique used for convert all the input variables to vary within a similar range of values.\n",
    "\n",
    "1. Normalization: eg. MinMaxScaler - range (-1,1) or (0,1) - outlier effect still remains the same.       \n",
    "    x_new = (x - xmin)/(xmax - xmin)\n",
    "                  \n",
    "2. Standardization: eg. Standard Scaler - range is variable, mean 0 and std of 1. - outliers are considered and skews and bell curve.        \n",
    "    x_new = (x - mean)/(std)\n",
    "               \n",
    "3. Robust Scaler: eg. RobustScaler - bell curve with outliers scaled to be outliers in the shrinked range. bell curve is not skewed.         \n",
    "    x_new = (x - median)/(Q75 - Q25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y  = make_classification(n_samples = 1000, n_features = 20, \n",
    "                            n_informative = 10, n_classes = 4)\n",
    "\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# diabetes\n",
    "X, y = load_diabetes(return_X_y=True, as_frame = True)\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "1. Ordinal encoding: OrdinalEncoder - natural order preserved, LabelEncoder - for single feature or target variable.\n",
    "2. Nominal Encoding: OneHotEncoder - no order, Dummy Variable Encoding - removes redundancy to dome extent\n",
    "3. Discretization: encoding continuos numerical values to ranked bins of discrete numbers.           \n",
    "    Eg. a numerical variable between 1 and 10 can be divided into an ordinal variable with 5 labels with an ordinal relationship 1 2 3 4 5 7 8 9 10 This is called Discretization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "X = np.array([['red'],['blue'],['green']])\n",
    "encoder = OrdinalEncoder()\n",
    "X_ = encoder.fit_transform(X)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kruth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse = False)\n",
    "X_ = encoder.fit_transform(X)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kruth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False, drop = 'first')\n",
    "X_ = encoder.fit_transform(X)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kruth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "X_ = encoder.fit_transform(X)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making distributions more Guassian\n",
    "Removes skewness in the distribution      \n",
    "Using PowerTransformer class:              \n",
    "We can apply a power transform directly by calculating the log or squareroot of the variable.       \n",
    "lambda values: 0.0 for log, -0.5 for reciprocal squareroot, 0.5 for squareroot, etc.         \n",
    "Methods: 'box-cox' and 'yeo-johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "X, y  = make_classification(n_samples = 1000, n_features = 20, \n",
    "                            n_informative = 10, n_classes = 4)\n",
    "\n",
    "trans = PowerTransformer(method = 'yeo-johnson', standardize=True)\n",
    "X_ = trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Numerical Data Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile Trasform:        \n",
    "The quantile function ranks or smooths out the relationship between observations\n",
    "and can be mapped onto other distributions, such as the uniform or normal\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "X_ = trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Dimentionslity Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features\n",
    "\n",
    "A squared or cubed version of an input variable will change the\n",
    "probability distribution, separating the small and large values, a\n",
    "separation that is increased with the size of the exponent\n",
    "               \n",
    "This separation can help some machine learning algorithms make\n",
    "better predictions and is common for regression predictive modeling\n",
    "tasks and generally tasks that have numerical input variables\n",
    "                    \n",
    "Typically linear algorithms, such as linear regression and logistic\n",
    "regression, respond well to the use of polynomial input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n",
      "(1000, 231)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "trans = PolynomialFeatures(degree = 2) # will add 2 varibles for each input\n",
    "print(X.shape)\n",
    "X_ = trans.fit_transform(X)\n",
    "print(X_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "1. Wrapper method: Recursive Feature Elimination - wrapping a ML model and testing with different subsets of features.           \n",
    "2. Filter method: Pearson's Correlation and Chi-squared test - scoring methods - most predictive features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorisation\n",
    "\n",
    "1. EigenDecomposition\n",
    "2. SingularValueDecomposition\n",
    "3. PrincipalComponentAnalysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n",
    "\n",
    "Techniques from linear algebra can be used for dimensionality reduction           \n",
    "These techniques are sometimes referred to as manifold learning and are used to\n",
    "create a low dimensional projection of high dimensional data, often for the\n",
    "purposes of data visualization\n",
    "\n",
    "      \n",
    "Examples of manifold learning techniques include \n",
    "- Kohonen Self Organizing Map ( SOM)\n",
    "- Sammons Mapping \n",
    "- Multidimensional Scaling ( MDS)\n",
    "- t distributed Stochastic Neighbor Embedding (t SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Method\n",
    "\n",
    "Autoencoders involves in framing a self supervised learning problem\n",
    "where a model must reproduce the input correctly\n",
    "\n",
    "A network model is used that seeks to compress the data flow to a\n",
    "bottleneck layer with far fewer dimensions than the original input data\n",
    "\n",
    "The part of the model prior to and including the bottleneck is referred\n",
    "to as the encoder, and the part of the model that reads the bottleneck\n",
    "output and reconstructs the input is called the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "Specifically the model seeks to find a linear combination of input\n",
    "variables that achieves the maximum separation for samples between\n",
    "classes (class centroids or means) and the minimum separation of\n",
    "samples within each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
