{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Cross Validation and Splitting Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement K-fold cross validation techniques\n",
    "2. Implement the program to avoid Data leakage with Naive Data preparation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-fold cross validation technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Folds cross-validator:      \n",
    "Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).       \n",
    "Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    "\n",
    "methods:\n",
    "- split(X, y=None, groups=None), yields train and test data indices for each fold\n",
    "- get_n_splits(X=None, y=None, groups=None), returns the number of splitting iterations in the cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n",
      "[[10 20]\n",
      " [30 40]]\n",
      "[[100   2]\n",
      " [300 400]]\n",
      "[1 2]\n",
      "[3 4]\n",
      "(array([2, 3]), array([0, 1]))\n",
      "(array([0, 1]), array([2, 3]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# dataset in the form of numpy array\n",
    "X = np.array([[10, 20], [30, 40], [100, 2], [300, 400]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Define the split - into 2 folds, no shuffling by default\n",
    "kf = KFold(n_splits=2, shuffle=False, random_state=None) \n",
    "\n",
    "# get the indices of the split\n",
    "for train_index, test_index in kf.split(X):\n",
    "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "X_, y_= kf.split(X)\n",
    "print(X_)\n",
    "print(y_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive (simple) Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_classification method:\n",
    "\n",
    "Use make_classification() to create a variety of classification datasets.     \n",
    "Parameters:\n",
    "- n_samples: How many observations do you want to generate?\n",
    "- n_features: The number of numerical features.\n",
    "- n_informative: The number of features that are ‘useful.’ Only these features carry the signal that your model will use to classify the dataset.\n",
    "- n_classes: The number of unique classes (values) for the target label.\n",
    "- n_redundant: The number of redundant features (aka irrelevant features).  These features are generated as random linear combinations of the informative features.\n",
    "- random_state: Set this value for reproducibility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler method:\n",
    "\n",
    "- transform features by scaling each feature to a given range.\n",
    "- This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "- The transformation is given by:\n",
    "\n",
    "```\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "where min, max = feature_range.\n",
    "```\n",
    "- This transformation is often used as an alternative to zero mean, unit variance scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test_split method:\n",
    "Split arrays or matrices into random train and test subsets.\n",
    "Parameter:\n",
    "- test_size: represents the proportion of the dataset to include in the test set. default: 0.25\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Leakage in this case: (Wrong approach)\n",
    "fit_transform() on the whole dataset, then split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8484848484848485\n",
      "Accuracy: 84.848\n"
     ]
    }
   ],
   "source": [
    "# naive approach to normalizing the data before splitting the data and evaluating the model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# standardize the dataset\n",
    "# new_val = (old_val - min) / (max - min)\n",
    "scaler = MinMaxScaler() # values between 0 and 1\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "print(model.score(X_test,y_test))\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data leakage:\n",
    " refers to a problem where information about the holdout dataset, such as a test or validation dataset, is made available to the model in the training dataset. This leakage is often small and subtle but can have a marked effect on performance. A naive approach to preparing data applies the transform on the entire dataset before evaluating the performance of the model. This results in a problem referred to as data leakage, where knowledge of the hold-out test set leaks into the dataset used to train the model. This can result in an incorrect estimate of model performance when making predictions on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Without Data Leakage: (Right approach)\n",
    "after splitting datasets, fit() on the train set, then transform() on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.455\n"
     ]
    }
   ],
   "source": [
    "# correct approach for normalizing the data after the data is split before the model is evaluated\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# define the scaler\n",
    "# new_val = (old_val - min) / (max - min)\n",
    "scaler = MinMaxScaler() # values between 0 and 1\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
